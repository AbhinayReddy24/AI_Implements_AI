{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import runnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loading\n",
    "# loader = WebBaseLoader(\"/Users/abhinay/Downloads/titanic-data-science-solutions.html\") \n",
    "\n",
    "# pdf loading\n",
    "loader = PyPDFLoader(\"/Users/abhinay/Downloads/titanic-data-science-solutions.pdf\") # pdf loading\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store splits\n",
    "embeddings  = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=key)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a valid namespace with a prompt template\n",
    "prompt_namespace = {\n",
    "    'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],\n",
    "    'lc': 1,\n",
    "    'type': 'constructor',\n",
    "    'kwargs': {\n",
    "        'template': \"You are an assistant to get the required steps and code any to build Model based on given artile. Use the following pieces of retrieved context to answer the question if it is code and not asked for steps just give code nothing else also go to new line as it is mentioned in the article. If you don't know the answer, just say that you don't know. Use maximum of 10 sentence if needed more sentences use it.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
    "        'input_variables': ['question', 'context'],\n",
    "        'template_format': 'f-string'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create an instance of the prompt template\n",
    "prompt_template = PromptTemplate(**prompt_namespace['kwargs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=key, verbose=True)\n",
    "rag_prompt = PromptTemplate(**prompt_namespace['kwargs'])\n",
    "rag_chain = {\"context\": retriever, \"question\": runnable.RunnablePassthrough()} | rag_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(summary_text, words_per_line=8):\n",
    "    words = summary_text.split()\n",
    "    lines = [words[i:i+words_per_line] for i in range(0, len(words), words_per_line)]\n",
    "    formatted_text = '\\n'.join([' '.join(line) for line in lines])\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = rag_chain.invoke(\"what was the score of KNN model\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The list of models and their scores in tabular form is as follows: Model\n",
      "Score ------------------------- Random Forest 86.76 Decision Tree 86.76 KNN 84.74 Support Vector Machines 83.84\n",
      "Logistic Regression 80.36 Linear SVC 79.12 Stochastic Gradient Decent 78.56 Perceptron 78.00 Naive Bayes\n",
      "72.28\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"give me the list of models and their scores on tabular form\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest = RandomForestClassifier(n_estimators=100) random_forest.fit(X_train, Y_train) Y_pred = random_forest.predict(X_test) random_forest.score(X_train, Y_train) acc_random_forest = round(random_forest.score(X_train, Y_train)\n",
      "* 100, 2) acc_random_forest\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what was the code to implement random forest model \")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python libraries that were imported in the given article are: - pandas -\n",
      "sklearn.linear_model.LogisticRegression - sklearn.svm.SVC - sklearn.svm.LinearSVC - sklearn.ensemble.RandomForestClassifier - sklearn.neighbors.KNeighborsClassifier - sklearn.naive_bayes.GaussianNB - sklearn.linear_model.Perceptron -\n",
      "sklearn.linear_model.SGDClassifier - sklearn.tree.DecisionTreeClassifier\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were all the python libraries that was imported\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aim of this paper is to explain the step-by-step process of building an\n",
      "end-to-end data pipeline for the Kaggle Titanic competition. The author wants to help readers\n",
      "understand the structured approach of a Machine Learning Engineer or Data Scientist in data\n",
      "cleaning and feature engineering. The paper also aims to provide insights on how to\n",
      "improve the model and make it more difficult for future participants to stand out.\n",
      "The author suggests exploring more data imputation possibilities, trying different classifiers such as XGBoost,\n",
      "and combining several classifiers using a voting pipeline. Overall, the goal is to achieve\n",
      "a high score and rank among the top 3% in the competition.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what was the aim of this paper or what they are trying to achieve\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main headings in the article are: 1. Count plot for titles after feature\n",
      "engineering 2. Count plot after grouping titles to frequent groups 3. Bar plot survival\n",
      "rate vs title 4. Conclusion\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what are main headings in the article\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key takeaways from the exploratory data analysis are: 1. Exploratory data analysis is\n",
      "time-consuming but essential in understanding the dataset and asking the right questions. 2. The\n",
      "analysis helps in identifying weak features that can be eliminated and focusing on meaningful\n",
      "features. 3. The cabin feature in the Titanic dataset can be improved by considering\n",
      "the number of cabins booked instead of just extracting the first two letters. 4.\n",
      "Balancing the dataset and trying different metrics can be explored to improve the model.\n",
      "5. The thought process of a Machine Learning Engineer/Data Scientist in data cleaning and\n",
      "feature engineering is crucial for achieving top scores. 6. The complete code for the\n",
      "analysis can be found on the GitHub repository provided. 7. Getting the data and\n",
      "importing necessary modules for handling tabular data is the first step in the process.\n",
      "8. Exploratory data analysis involves investigating the data and understanding the available features. 9.\n",
      "Not all features are necessary for the analysis, and many of them may not\n",
      "contribute significantly to the model. 10. The goal is to identify the most relevant\n",
      "features that can be used to build an accurate predictive model.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the key take aways from the exploratory data analysis give detailed explaination\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key features that can be used to train the model based on exploratory\n",
      "data analysis are not explicitly mentioned in the given context. However, the context suggests\n",
      "that the analysis aims to reduce the number of weak features and identify meaningful\n",
      "features. It also mentions the possibility of extracting the number of cabins booked as\n",
      "a potential feature. Additionally, it emphasizes the importance of exploring which features should be\n",
      "considered and which should not. Therefore, further analysis and investigation of the available features\n",
      "are required to determine the specific key features for training the model.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"based on exploratory data anaylyis what are the key features that can be used to trian the model\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, the key features that can be used to train\n",
      "the model are: 1. Number of cabins booked: Instead of simply extracting the first\n",
      "two letters of the cabin feature, the number of cabins booked by a passenger\n",
      "can be considered as a possible feature. 2. Balancing the dataset: Ways to balance\n",
      "the dataset can be explored to improve the model's performance. 3. Different metric: Trying\n",
      "a different metric for evaluation can be considered to enhance the model's accuracy. 4.\n",
      "Exploratory data analysis: Conducting exploratory data analysis can help in reducing the number of\n",
      "weak features and identifying meaningful features. 5. Feature engineering: Creating new features such as\n",
      "extracting the number of cabins used and engineering ticket and cabin features can be\n",
      "experimented with. 6. Family size: Combining the features Parch and SibSp to create a\n",
      "new feature that captures the information of both can be useful. 7. Bar plot:\n",
      "Analyzing the bar plot of family size vs survival can provide insights into the\n",
      "relationship between these features. 8. Sparse feature space: It is important to explore which\n",
      "features should be considered and which should not, especially in a sparse feature space\n",
      "where ML algorithms tend to overfit to noise. 9. Being unbiased and asking good\n",
      "questions: Being unbiased and asking good questions is an important skill for a Machine\n",
      "Learning Engineer/Data Scientist. 10. Investigating the resulting dataframe: Analyzing the resulting dataframe after feature\n",
      "engineering, specifically the train_df, can provide insights into the effectiveness of the engineered features.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"based on feature engineering what are the key features that can be used to trian the model\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new features that were created in the feature engineering section are: 1. Number\n",
      "of cabins used 2. Cabin letter 3. Ticket and Cabin engineered new features\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the new features that were created in feature engineering section\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final features that were used in the training a classifier section were 'Pclass',\n",
      "'Fare', 'Title', 'Embarked', 'Fam_type', 'Ticket_len', and 'Ticket_2letter'. The 'Cabin' feature was not used, and\n",
      "the relevant information about the feature 'age' was already encoded in the 'title' feature.\n",
      "The 'Sex' feature was also not used to avoid confusing the classifier.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the final features that were used in the training a classifier section\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The steps involved in creating the pipeline for the model based on the given\n",
      "article are as follows: 1. Getting the data: The first step is to obtain\n",
      "the Titanic dataset, which is the dataset used in the Kaggle competition. This dataset\n",
      "will be used for training and testing the model. 2. Importing necessary modules: The\n",
      "required modules for handling tabular data and performing machine learning tasks are imported. These\n",
      "modules will provide the necessary functions and methods for data preprocessing, feature engineering, and\n",
      "model training. 3. Data cleaning and feature engineering: The thought process of a Machine\n",
      "Learning Engineer or Data Scientist in data cleaning and feature engineering is discussed. This\n",
      "step involves handling missing values, transforming categorical variables, creating new features, and preparing the\n",
      "data for model training. 4. Building the model: The specific model used in the\n",
      "article is not mentioned, but it is suggested that the Random Forest (RF) classifier\n",
      "is used. However, it is also mentioned that other classifiers like XGBoost can be\n",
      "tried. The model is trained on the preprocessed data. 5. Evaluating the model: The\n",
      "trained model is evaluated using appropriate evaluation metrics, such as accuracy, precision, recall, or\n",
      "F1 score. This step helps in assessing the performance of the model and identifying\n",
      "areas for improvement. 6. Making predictions: Once the model is trained and evaluated, it\n",
      "can be used to make predictions on new, unseen data. This step involves applying\n",
      "the trained model to the test data and generating predictions for survival on the\n",
      "Titanic. 7. Creating a submission file: The final step is to create and upload\n",
      "a submission file containing the predictions. This file will be used to evaluate the\n",
      "model's performance in the Kaggle competition. It is also mentioned that the presented pipeline\n",
      "is a starting point and there are many possibilities for improvement that have not\n",
      "been addressed in the article.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the steps involved in creating pipeline\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comments in the pipeline picture were not mentioned in the given context.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the comments in the pipline picture\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our evaluation metric is how close are we to final goal mentioned in the article\n",
    "# so first step should be to get the final goal of the paper which is very standard or we can even put it as constant prompt\n",
    "# we can feed the model with all the possible titanic aritcles \n",
    "# RAG is not perfect all the time and it might not be completly reliable all the time\n",
    "# Instruct fine tunning is kind of path way we need to take based on @daniel analysis\n",
    "# to develop instruct fine tunning we need data set of the below foramt\n",
    "\n",
    "# prompt: what are the steps I need to build ML model based on the below mentioned article\n",
    "# response: we need to ask the following questions:\n",
    "# Q1: what is this paper trying to achieve or what are the final outcomes\n",
    "# Q2: what is in the introduction\n",
    "# Q3: what is the first step taken (this can be data cleaning and managing data imbalances)\n",
    "# Q4: how first step is implemented (follow up question)\n",
    "# Q5: what was done after first step or what is the step 2 (this can be exploratory data analysis)\n",
    "# Q6: what was intrepreted from the step 2 (follow up question)\n",
    "# Q7: what was done after step 2 or what is step 3 (this can be feature engineering) - (there might not be this step in all \n",
    "#       the papers)\n",
    "# Q8: was step 2 (EDA) had affect on step 3 (this can be achieved by verifying if there are new features intrudouced compared to \n",
    "#       initial features)\n",
    "# Q9: what are the outcomes of step 3 (final features from step 3)\n",
    "# Q10:what is the done after step 3 or what is step 4 (this can what was the baseline model selected) or (the models they played \n",
    "#       with before goining to final model)\n",
    "# Q11:what are the evaluation metrics (this might not be a step this can asked at any point preferebly before baseline model)\n",
    "# Q12:what is the main model they fouced on or step 5  - at this point I am not sure to call it step 5 but let's keep it that way\n",
    "# Q13:how the model is tuned or what parameters that were tuned \n",
    "# Q14:what were the final tuned parameters \n",
    "# Q15:what is the accuracy of the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# if we are trying to implement the paper we should not worry about how much data we have because who ever worked on that paper \n",
    "# would have taken care of it\n",
    "# we need to focus on how they implemented the startiges to make use of that data or how they processed it (which is kind of \n",
    "#   abstract view of data prepration step)\n",
    "#  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Templet:**\n",
    "You are an assistant to get the required steps to build Model based on given artile. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use maximum of 10 sentence if needed more sentences use it\n",
    "\n",
    "**Question:**\n",
    "what are the things that I need to do in Exploratory data analysis ?\n",
    "\n",
    "**Answer:**\n",
    "In exploratory data analysis, there are several steps that need to be taken. First, you need to import the necessary modules for handling tabular data. Then, you should read the data CSV files and convert them into pandas dataframes. After that, you can investigate the data to gain a better understanding of the available features. It is not necessary to use every available feature, so you should identify the relevant ones and discard the irrelevant ones. This can help reduce the number of weak features and focus on creating meaningful features. Finally, you can perform data cleaning and feature engineering to improve the quality of the data and create new features that can enhance the performance of the model.\n",
    "\n",
    "**Templet:**\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "**Question:**\n",
    "what are the things that I need to do in Exploratory data analysis ?\n",
    "\n",
    "**Answer:**\n",
    "In exploratory data analysis, you need to ask the right questions and spend time\n",
    "searching for better features. The goal is to reduce weak features and create meaningful\n",
    "ones. Preprocessing the data and understanding the available features are also important steps in\n",
    "EDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
