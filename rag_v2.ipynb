{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import runnable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web based loading\n",
    "loader = WebBaseLoader(\"https://anelmusic13.medium.com/how-to-score-top-3-in-kaggles-titanic-machine-learning-from-disaster-competition-13d056e262b1\") \n",
    "\n",
    "# pdf loading\n",
    "# loader = PyPDFLoader(\"/Users/abhinay/Downloads/Real-Time_Detection_of_DNS_Exfiltration_and_Tunneling_from_Enterprise_Networks-1.pdf\") # pdf loading\n",
    "# pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"no key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store splits\n",
    "embeddings  = OpenAIEmbeddings(model=\"text-embedding-ada-002\", openai_api_key=key)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a valid namespace with a prompt template\n",
    "prompt_namespace = {\n",
    "    'id': ['langchain', 'prompts', 'prompt', 'PromptTemplate'],\n",
    "    'lc': 1,\n",
    "    'type': 'constructor',\n",
    "    'kwargs': {\n",
    "        'template': \"You are an assistant to get the required steps to build Model based on given artile. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use maximum of 10 sentence if needed more sentences use it.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
    "        'input_variables': ['question', 'context'],\n",
    "        'template_format': 'f-string'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create an instance of the prompt template\n",
    "prompt_template = PromptTemplate(**prompt_namespace['kwargs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=key, verbose=True)\n",
    "rag_prompt = PromptTemplate(**prompt_namespace['kwargs'])\n",
    "rag_chain = {\"context\": retriever, \"question\": runnable.RunnablePassthrough()} | rag_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(summary_text, words_per_line=8):\n",
    "    words = summary_text.split()\n",
    "    lines = [words[i:i+words_per_line] for i in range(0, len(words), words_per_line)]\n",
    "    formatted_text = '\\n'.join([' '.join(line) for line in lines])\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In exploratory data analysis, the first step is to import the necessary modules for\n",
      "handling tabular data. Then, you need to read the data CSV files and convert\n",
      "them into pandas dataframes. After that, you can investigate the data to gain a\n",
      "better understanding of the available features. It is not necessary to use every available\n",
      "feature, so you should identify the relevant ones. This analysis helps in reducing the\n",
      "number of weak features and selecting meaningful ones. Finally, you can start exploring and\n",
      "visualizing the data to uncover patterns, relationships, and potential outliers.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what are the things that I need to do in Exploratory data analysis\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The article is trying to achieve the goal of explaining the step-by-step process of\n",
      "building an end-to-end data pipeline for the Kaggle competition \"Titanic - Machine Learning from\n",
      "Disaster.\" It aims to provide insights and guidance on how to score in the\n",
      "top 3% of participants by discussing the thought process of a Machine Learning Engineer/Data\n",
      "Scientist in data cleaning and feature engineering. The article also emphasizes the importance of\n",
      "exploratory data analysis and suggests ways to improve the model's performance by considering additional\n",
      "features and balancing the dataset. Overall, the article aims to help readers understand the\n",
      "structured approach to machine learning and improve their performance in the competition.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what is this article trying to achieve\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The questions mentioned in the Exploratory data analysis are: 1. Does the dataset contain\n",
      "missing values? 2. How many entries are missing in the dataset? 3. Which features\n",
      "have missing values? 4. How can we solve the problem of missing values? 5.\n",
      "Could the place of embarkation influence the chance of survival? 6. How can we\n",
      "analyze the influence of the place of embarkation on survival? 7. What are the\n",
      "different locations of embarkation? 8. What are the count plots for the different locations\n",
      "of embarkation? 9. How can we preprocess the data before training the model? 10.\n",
      "What is the purpose of exploratory data analysis (EDA)?\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what are the questions mentioned in the Exploratory data analysis√†\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction of the article discusses the Titanic dataset and its relevance to the\n",
      "Kaggle competition. It mentions that the competition is about predicting whether a passenger survived\n",
      "the disaster or not. The article also states that the most important factor for\n",
      "survival is gender, followed by class. It raises the question of why the survival\n",
      "rate for Queenstown passengers is slightly higher than for Southampton passengers, despite Southampton having\n",
      "a higher ratio of first-class passengers. The introduction sets the stage for exploring the\n",
      "factors that influence survival in the Titanic dataset.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what was in the introduction give detailed explaination\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The aim of this paper is to explain the step-by-step process of building an\n",
      "end-to-end data pipeline for the Kaggle Titanic competition. The author wants to help readers\n",
      "understand the structured approach of a Machine Learning Engineer or Data Scientist in data\n",
      "cleaning and feature engineering. The paper also aims to provide insights on how to\n",
      "improve the model and make it more difficult for future participants to stand out.\n",
      "The author suggests exploring more data imputation possibilities, trying different classifiers such as XGBoost,\n",
      "and combining several classifiers using a voting pipeline. Overall, the goal is to achieve\n",
      "a high score and rank among the top 3% in the competition.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what was the aim of this paper or what they are trying to achieve\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main headings in the article are: 1. Count plot for titles after feature\n",
      "engineering 2. Count plot after grouping titles to frequent groups 3. Bar plot survival\n",
      "rate vs title 4. Conclusion\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what are main headings in the article\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key takeaways from the exploratory data analysis are: 1. Exploratory data analysis is\n",
      "time-consuming but essential in understanding the dataset and asking the right questions. 2. The\n",
      "analysis helps in identifying weak features that can be eliminated and focusing on meaningful\n",
      "features. 3. The cabin feature in the Titanic dataset can be improved by considering\n",
      "the number of cabins booked instead of just extracting the first two letters. 4.\n",
      "Balancing the dataset and trying different metrics can be explored to improve the model.\n",
      "5. The thought process of a Machine Learning Engineer/Data Scientist in data cleaning and\n",
      "feature engineering is crucial for achieving top scores. 6. The complete code for the\n",
      "analysis can be found on the GitHub repository provided. 7. Getting the data and\n",
      "importing necessary modules for handling tabular data is the first step in the process.\n",
      "8. Exploratory data analysis involves investigating the data and understanding the available features. 9.\n",
      "Not all features are necessary for the analysis, and many of them may not\n",
      "contribute significantly to the model. 10. The goal is to identify the most relevant\n",
      "features that can be used to build an accurate predictive model.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the key take aways from the exploratory data analysis give detailed explaination\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key features that can be used to train the model based on exploratory\n",
      "data analysis are not explicitly mentioned in the given context. However, the context suggests\n",
      "that the analysis aims to reduce the number of weak features and identify meaningful\n",
      "features. It also mentions the possibility of extracting the number of cabins booked as\n",
      "a potential feature. Additionally, it emphasizes the importance of exploring which features should be\n",
      "considered and which should not. Therefore, further analysis and investigation of the available features\n",
      "are required to determine the specific key features for training the model.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"based on exploratory data anaylyis what are the key features that can be used to trian the model\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given context, the key features that can be used to train\n",
      "the model are: 1. Number of cabins booked: Instead of simply extracting the first\n",
      "two letters of the cabin feature, the number of cabins booked by a passenger\n",
      "can be considered as a possible feature. 2. Balancing the dataset: Ways to balance\n",
      "the dataset can be explored to improve the model's performance. 3. Different metric: Trying\n",
      "a different metric for evaluation can be considered to enhance the model's accuracy. 4.\n",
      "Exploratory data analysis: Conducting exploratory data analysis can help in reducing the number of\n",
      "weak features and identifying meaningful features. 5. Feature engineering: Creating new features such as\n",
      "extracting the number of cabins used and engineering ticket and cabin features can be\n",
      "experimented with. 6. Family size: Combining the features Parch and SibSp to create a\n",
      "new feature that captures the information of both can be useful. 7. Bar plot:\n",
      "Analyzing the bar plot of family size vs survival can provide insights into the\n",
      "relationship between these features. 8. Sparse feature space: It is important to explore which\n",
      "features should be considered and which should not, especially in a sparse feature space\n",
      "where ML algorithms tend to overfit to noise. 9. Being unbiased and asking good\n",
      "questions: Being unbiased and asking good questions is an important skill for a Machine\n",
      "Learning Engineer/Data Scientist. 10. Investigating the resulting dataframe: Analyzing the resulting dataframe after feature\n",
      "engineering, specifically the train_df, can provide insights into the effectiveness of the engineered features.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"based on feature engineering what are the key features that can be used to trian the model\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new features that were created in the feature engineering section are: 1. Number\n",
      "of cabins used 2. Cabin letter 3. Ticket and Cabin engineered new features\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the new features that were created in feature engineering section\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final features that were used in the training a classifier section were 'Pclass',\n",
      "'Fare', 'Title', 'Embarked', 'Fam_type', 'Ticket_len', and 'Ticket_2letter'. The 'Cabin' feature was not used, and\n",
      "the relevant information about the feature 'age' was already encoded in the 'title' feature.\n",
      "The 'Sex' feature was also not used to avoid confusing the classifier.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the final features that were used in the training a classifier section\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The steps involved in creating the pipeline for the model based on the given\n",
      "article are as follows: 1. Getting the data: The first step is to obtain\n",
      "the Titanic dataset, which is the dataset used in the Kaggle competition. This dataset\n",
      "will be used for training and testing the model. 2. Importing necessary modules: The\n",
      "required modules for handling tabular data and performing machine learning tasks are imported. These\n",
      "modules will provide the necessary functions and methods for data preprocessing, feature engineering, and\n",
      "model training. 3. Data cleaning and feature engineering: The thought process of a Machine\n",
      "Learning Engineer or Data Scientist in data cleaning and feature engineering is discussed. This\n",
      "step involves handling missing values, transforming categorical variables, creating new features, and preparing the\n",
      "data for model training. 4. Building the model: The specific model used in the\n",
      "article is not mentioned, but it is suggested that the Random Forest (RF) classifier\n",
      "is used. However, it is also mentioned that other classifiers like XGBoost can be\n",
      "tried. The model is trained on the preprocessed data. 5. Evaluating the model: The\n",
      "trained model is evaluated using appropriate evaluation metrics, such as accuracy, precision, recall, or\n",
      "F1 score. This step helps in assessing the performance of the model and identifying\n",
      "areas for improvement. 6. Making predictions: Once the model is trained and evaluated, it\n",
      "can be used to make predictions on new, unseen data. This step involves applying\n",
      "the trained model to the test data and generating predictions for survival on the\n",
      "Titanic. 7. Creating a submission file: The final step is to create and upload\n",
      "a submission file containing the predictions. This file will be used to evaluate the\n",
      "model's performance in the Kaggle competition. It is also mentioned that the presented pipeline\n",
      "is a starting point and there are many possibilities for improvement that have not\n",
      "been addressed in the article.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the steps involved in creating pipeline\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comments in the pipeline picture were not mentioned in the given context.\n"
     ]
    }
   ],
   "source": [
    "summary = rag_chain.invoke(\"what were the comments in the pipline picture\")\n",
    "formatted_summary = format_output(summary.content, words_per_line=14)\n",
    "print(formatted_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our evaluation metric is how close are we to final goal mentioned in the article\n",
    "# so first step should be to get the final goal of the paper which is very standard or we can even put it as constant prompt\n",
    "# we can feed the model with all the possible titanic aritcles \n",
    "# RAG is not perfect all the time and it might not be completly reliable all the time\n",
    "# Instruct fine tunning is kind of path way we need to take based on @daniel analysis\n",
    "# to develop instruct fine tunning we need data set of the below foramt\n",
    "\n",
    "# prompt: what are the steps I need to build ML model based on the below mentioned article\n",
    "# response: we need to ask the following questions:\n",
    "# Q1: what is this paper trying to achieve or what are the final outcomes\n",
    "# Q2: what is in the introduction\n",
    "# Q3: what is the first step taken (this can be data cleaning and managing data imbalances)\n",
    "# Q4: how first step is implemented (follow up question)\n",
    "# Q5: what was done after first step or what is the step 2 (this can be exploratory data analysis)\n",
    "# Q6: what was intrepreted from the step 2 (follow up question)\n",
    "# Q7: what was done after step 2 or what is step 3 (this can be feature engineering) - (there might not be this step in all \n",
    "#       the papers)\n",
    "# Q8: was step 2 (EDA) had affect on step 3 (this can be achieved by verifying if there are new features intrudouced compared to \n",
    "#       initial features)\n",
    "# Q9: what are the outcomes of step 3 (final features from step 3)\n",
    "# Q10:what is the done after step 3 or what is step 4 (this can what was the baseline model selected) or (the models they played \n",
    "#       with before goining to final model)\n",
    "# Q11:what are the evaluation metrics (this might not be a step this can asked at any point preferebly before baseline model)\n",
    "# Q12:what is the main model they fouced on or step 5  - at this point I am not sure to call it step 5 but let's keep it that way\n",
    "# Q13:how the model is tuned or what parameters that were tuned \n",
    "# Q14:what were the final tuned parameters \n",
    "# Q15:what is the accuracy of the final model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES\n",
    "# if we are trying to implement the paper we should not worry about how much data we have because who ever worked on that paper \n",
    "# would have taken care of it\n",
    "# we need to focus on how they implemented the startiges to make use of that data or how they processed it (which is kind of \n",
    "#   abstract view of data prepration step)\n",
    "#  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Templet:**\n",
    "You are an assistant to get the required steps to build Model based on given artile. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use maximum of 10 sentence if needed more sentences use it\n",
    "\n",
    "**Question:**\n",
    "what are the things that I need to do in Exploratory data analysis ?\n",
    "\n",
    "**Answer:**\n",
    "In exploratory data analysis, there are several steps that need to be taken. First, you need to import the necessary modules for handling tabular data. Then, you should read the data CSV files and convert them into pandas dataframes. After that, you can investigate the data to gain a better understanding of the available features. It is not necessary to use every available feature, so you should identify the relevant ones and discard the irrelevant ones. This can help reduce the number of weak features and focus on creating meaningful features. Finally, you can perform data cleaning and feature engineering to improve the quality of the data and create new features that can enhance the performance of the model.\n",
    "\n",
    "**Templet:**\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "**Question:**\n",
    "what are the things that I need to do in Exploratory data analysis ?\n",
    "\n",
    "**Answer:**\n",
    "In exploratory data analysis, you need to ask the right questions and spend time\n",
    "searching for better features. The goal is to reduce weak features and create meaningful\n",
    "ones. Preprocessing the data and understanding the available features are also important steps in\n",
    "EDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
